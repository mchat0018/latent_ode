{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent-ODE to generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import umap\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import SystemRandom\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "import torch.optim as optim\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.plotting import *\n",
    "\n",
    "from lib.ode_rnn import *\n",
    "from lib.create_latent_ode_model import create_LatentODE_model\n",
    "from lib.parse_datasets import parse_datasets\n",
    "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "\n",
    "from lib.utils import compute_loss_all_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Arguments for Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"train_miss_rate\": 40,\n",
    "    \"test_miss_rate\": 60,\n",
    "    \"batch_size\": 40,\n",
    "    \"num_workers\": 2,\n",
    "    \"save\": \"experiments\",\n",
    "    \"load\": 3,\n",
    "    \"n\": 8000,\n",
    "    \"classif\": False,\n",
    "    \"latents\": 3,\n",
    "    \"rec_dims\": 5,\n",
    "    \"poisson\": False,\n",
    "    \"gen_layers\": 2,\n",
    "    \"rec_layers\": 2,\n",
    "    \"units\": 20,\n",
    "    \"gru_units\":20,\n",
    "    \"z0_encoder\": \"odernn\",\n",
    "    \"lr\": 1e-2,\n",
    "    \"epochs\": 20,\n",
    "    \"chkpt\": 5\n",
    "}\n",
    "args = dotdict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting files and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(seed=42)\n",
    "np.random.seed(seed=10)\n",
    "\n",
    "# file_name = os.path.basename(__file__)[:-3]\n",
    "save_path = args.save\n",
    "utils.makedirs(save_path)\n",
    "\n",
    "experimentID = args.load\n",
    "if experimentID is None:\n",
    "    # Make a new experiment ID\n",
    "    experimentID = int(SystemRandom().random()*100000)\n",
    "ckpt_path = os.path.join(save_path, \"experiment_\" + str(experimentID))\n",
    "if not os.path.exists(ckpt_path):\n",
    "    os.makedirs(ckpt_path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.preprocess_data import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 1000\n",
    "TIMESTAMPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling dataset of 8000 training examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Mihir\\Ashoka RAship\\ESIMC Datathon\\Code\\Trajectory\\latent_ode\\lib\\preprocess_data.py:10: RuntimeWarning: invalid value encountered in divide\n",
      "  data_norm = np.nan_to_num((data - data_min) / (data_max - data_min))\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"Sampling dataset of {} training examples\".format(args.n))\n",
    "utils.makedirs(\"results/\")\n",
    "\n",
    "data_obj = preprocess(args)\n",
    "input_dim = data_obj[\"input_dim\"]\n",
    "\n",
    "classif_per_tp = data_obj[\"classif_per_tp\"] if \"classif_per_tp\" in data_obj else False\n",
    "n_labels = -1\n",
    "if args.classif:\n",
    "    if (\"n_labels\" in data_obj): n_labels = data_obj[\"n_labels\"]\n",
    "    else: raise Exception(\"Please provide number of labels for classification task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_obj': <torch.utils.data.dataset.TensorDataset at 0x273168bfd60>,\n",
       " 'train_dataloader': <torch.utils.data.dataloader.DataLoader at 0x27334916290>,\n",
       " 'test_dataloader': <torch.utils.data.dataloader.DataLoader at 0x27334915de0>,\n",
       " 'input_dim': 4,\n",
       " 'n_train_batches': 20,\n",
       " 'n_test_batches': 5,\n",
       " 'classif_per_tp': False,\n",
       " 'n_labels': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsrv_std = 0.01\n",
    "obsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
    "\n",
    "z0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
    "\n",
    "model = create_LatentODE_model(\n",
    "    args, input_dim, z0_prior, obsrv_std, device, \n",
    "    classif_per_tp = classif_per_tp, n_labels = n_labels\n",
    ")\n",
    "\n",
    "if args.viz: viz = Visualizations(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentODE(\n",
       "  (encoder_z0): Encoder_z0_ODE_RNN(\n",
       "    (GRU_update): GRU_unit(\n",
       "      (update_gate): Sequential(\n",
       "        (0): Linear(in_features=18, out_features=20, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "      (reset_gate): Sequential(\n",
       "        (0): Linear(in_features=18, out_features=20, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "      (new_state_net): Sequential(\n",
       "        (0): Linear(in_features=18, out_features=20, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=20, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (z0_diffeq_solver): DiffeqSolver(\n",
       "      (ode_func): ODEFunc(\n",
       "        (gradient_net): Sequential(\n",
       "          (0): Linear(in_features=5, out_features=20, bias=True)\n",
       "          (1): Tanh()\n",
       "          (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "          (3): Tanh()\n",
       "          (4): Linear(in_features=20, out_features=20, bias=True)\n",
       "          (5): Tanh()\n",
       "          (6): Linear(in_features=20, out_features=5, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transform_z0): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=100, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=100, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (diffeq_solver): DiffeqSolver(\n",
       "    (ode_func): ODEFunc(\n",
       "      (gradient_net): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=20, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=20, out_features=20, bias=True)\n",
       "        (5): Tanh()\n",
       "        (6): Linear(in_features=20, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Mihir\\Ashoka RAship\\ESIMC Datathon\\Code\\Trajectory\\latent_ode\n",
      "Run_3\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.basename(os.curdir)[:-3]\n",
    "# setting logs\n",
    "log_path = \"logs/\" + file_name + \"_\" + str(experimentID) + \".log\"\n",
    "if not os.path.exists(\"logs/\"):\n",
    "    utils.makedirs(\"logs/\")\n",
    "logger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(os.curdir))\n",
    "logger.info(f\"Run_{experimentID}\")\n",
    "\n",
    "# training parameters\n",
    "optimizer = optim.Adamax(model.parameters(), lr=args.lr)\n",
    "num_batches = data_obj[\"n_train_batches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(kl_coef): \n",
    "    train_batch_loss = []\n",
    "    train_batch_mse = []\n",
    "    train_res_update = []\n",
    "    for batch_no, (data_batch,mask_batch,reconst_batch) in enumerate(data_obj['train_dataloader']):\n",
    "        optimizer.zero_grad()\n",
    "        utils.update_learning_rate(optimizer, decay_rate = 0.999, lowest = args.lr / 10)\n",
    "\n",
    "        batch_dict = {\n",
    "            \"tp_to_predict\": torch.tensor(np.arange(0,TIMESTAMPS,1),dtype=torch.float32),\n",
    "            \"observed_data\": data_batch.to(device),\n",
    "            \"observed_tp\": torch.tensor(np.arange(0,TIMESTAMPS,1),dtype=torch.float32),\n",
    "            \"observed_mask\": mask_batch.to(device),\n",
    "            \"data_to_predict\": reconst_batch.to(device),\n",
    "            \"mask_predicted_data\": None,\n",
    "            \"labels\": None\n",
    "        }\n",
    "        train_res = model.compute_all_losses(batch_dict, n_traj_samples = 3, kl_coef = kl_coef)\n",
    "        train_res[\"loss\"].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_batch_loss.append(train_res[\"loss\"].detach())\n",
    "        train_batch_mse.append(train_res[\"mse\"])\n",
    "        train_res_update = train_res\n",
    "\n",
    "    return train_batch_loss, train_batch_mse, train_res_update\n",
    "\n",
    "def test(kl_coef):\n",
    "    test_batch_loss = []\n",
    "    test_batch_mse = []\n",
    "    test_res_update = None\n",
    "    for batch_no, (data_batch,mask_batch,reconst_batch) in enumerate(data_obj['test_dataloader']):\n",
    "        with torch.no_grad():\n",
    "            batch_dict = {\n",
    "                \"tp_to_predict\": torch.tensor(np.arange(0,TIMESTAMPS,1),dtype=torch.float32),\n",
    "                \"observed_data\": data_batch.to(device),\n",
    "                \"observed_tp\": torch.tensor(np.arange(0,TIMESTAMPS,1),dtype=torch.float32),\n",
    "                \"observed_mask\": mask_batch.to(device),\n",
    "                \"data_to_predict\": reconst_batch.to(device),\n",
    "                \"mask_predicted_data\": None,\n",
    "                \"labels\": None\n",
    "            }\n",
    "            test_res = model.compute_all_losses(batch_dict, n_traj_samples = 3, kl_coef = kl_coef)\n",
    "            test_batch_loss.append(test_res[\"loss\"].detach())\n",
    "            test_batch_mse.append(test_res[\"mse\"])\n",
    "\n",
    "            test_res_update = test_res\n",
    "\n",
    "    return test_batch_loss, test_batch_mse, test_res_update\n",
    "\n",
    "def fit():\n",
    "    train_epoch_loss = []\n",
    "    train_epoch_mse = []\n",
    "    latest_embeddings = None\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        wait_until_kl_inc = 10\n",
    "        kl_coef = 0 if epoch < wait_until_kl_inc else (1-0.99**(epoch - wait_until_kl_inc))\n",
    "        train_batch_loss, train_batch_mse, train_res_update = train(kl_coef)\n",
    "        train_epoch_loss.append(np.mean(np.array(train_batch_loss)))\n",
    "        train_epoch_mse.append(np.mean(np.array(train_batch_mse)))\n",
    "\n",
    "        if args.chkpt and (epoch+1) % args.chkpt == 0:\n",
    "            print(f\"Checkpoint {(epoch+1) // args.chkpt} reached...\")\n",
    "            test_batch_loss, test_batch_mse, test_res_update = test(kl_coef)\n",
    "            message = 'Epoch {:04d} [Test seq (cond on sampled tp)] | Loss {:.6f} | Likelihood {:.6f} | KL fp {:.4f} | FP STD {:.4f}|'.format(\n",
    "                epoch+1, test_res_update[\"loss\"].detach(), test_res_update[\"likelihood\"].detach(), test_res_update[\"kl_first_p\"], test_res_update[\"std_first_p\"]\n",
    "            )\n",
    "    \n",
    "            logger.info(\"Experiment \" + str(experimentID))\n",
    "            logger.info(message)\n",
    "            logger.info(\"KL coef: {}\".format(kl_coef))\n",
    "            logger.info(\"Train loss (one batch): {}\".format(train_res_update[\"loss\"].detach()))\n",
    "            logger.info(\"Test MSE: {:.4f}\".format(test_res_update[\"mse\"]))\n",
    "\n",
    "            latest_embeddings = test_res_update[\"latent_variables\"].detach().numpy()\n",
    "            torch.save(model.state_dict(), os.path.join(ckpt_path,f\"state_{epoch}.pth\")) \n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(ckpt_path,\"final_model.pth\"))     \n",
    "    return train_epoch_loss, train_epoch_mse, latest_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1 reached...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 3\n",
      "Epoch 0005 [Test seq (cond on sampled tp)] | Loss 309.338898 | Likelihood -310.437531 | KL fp 5.4295 | FP STD 0.0169|\n",
      "KL coef: 0\n",
      "Train loss (one batch): 310.05780029296875\n",
      "Test MSE: 0.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 2 reached...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 3\n",
      "Epoch 0010 [Test seq (cond on sampled tp)] | Loss 309.144623 | Likelihood -310.243256 | KL fp 4.7811 | FP STD 0.0278|\n",
      "KL coef: 0\n",
      "Train loss (one batch): 309.6271667480469\n",
      "Test MSE: 0.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 3 reached...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 3\n",
      "Epoch 0015 [Test seq (cond on sampled tp)] | Loss 309.253998 | Likelihood -310.202423 | KL fp 3.8181 | FP STD 0.0653|\n",
      "KL coef: 0.039403990000000055\n",
      "Train loss (one batch): 309.8526611328125\n",
      "Test MSE: 0.0628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 4 reached...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 3\n",
      "Epoch 0020 [Test seq (cond on sampled tp)] | Loss 309.340240 | Likelihood -310.160583 | KL fp 3.2259 | FP STD 0.1126|\n",
      "KL coef: 0.08648275251635917\n",
      "Train loss (one batch): 311.0486145019531\n",
      "Test MSE: 0.0628\n"
     ]
    }
   ],
   "source": [
    "train_epoch_loss, train_epoch_mse, latest_embeddings = fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 40, 500, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
